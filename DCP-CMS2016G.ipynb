{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14832555,"sourceType":"datasetVersion","datasetId":9468572}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CMS2016G — Data Cleaning & Preprocessing (DCP) Audit Notebook","metadata":{}},{"cell_type":"markdown","source":"# Install Required Libraries\n\nWe install:\n- `uproot` → to read CMS ROOT files without ROOT framework\n- `awkward` → to handle jagged arrays from NanoAOD/MiniAOD\n\nThese are lightweight and optimized for HEP-scale datasets.\n","metadata":{}},{"cell_type":"code","source":"!pip install uproot awkward --quiet\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:19:31.353601Z","iopub.execute_input":"2026-02-14T06:19:31.353787Z","iopub.status.idle":"2026-02-14T06:19:38.490034Z","shell.execute_reply.started":"2026-02-14T06:19:31.353758Z","shell.execute_reply":"2026-02-14T06:19:38.489313Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.8/393.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m919.6/919.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m656.7/656.7 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import uproot\nimport awkward as ak\nimport numpy as np\nimport pandas as pd\nimport os\nfrom glob import glob\nimport math\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:19:38.492252Z","iopub.execute_input":"2026-02-14T06:19:38.492894Z","iopub.status.idle":"2026-02-14T06:19:39.129559Z","shell.execute_reply.started":"2026-02-14T06:19:38.492864Z","shell.execute_reply":"2026-02-14T06:19:39.128778Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Load CMS ROOT Files\n\nWe scan the dataset directory and load all ROOT files.\n\nExpected:\n- 85 ROOT files\n- Each containing a CMS event tree\n\nThis ensures:\n- No file is missing\n- No naming inconsistency\n- Dataset integrity across splits\n","metadata":{}},{"cell_type":"code","source":"BASE_PATH = \"/kaggle/input/datasets/hiteshrs/cms2016g29-5785\"\n\nroot_files = sorted(glob(os.path.join(BASE_PATH, \"*.root\")))\n\nprint(\"Total ROOT files found:\", len(root_files))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:19:39.133136Z","iopub.execute_input":"2026-02-14T06:19:39.133368Z","iopub.status.idle":"2026-02-14T06:19:39.146157Z","shell.execute_reply.started":"2026-02-14T06:19:39.133347Z","shell.execute_reply":"2026-02-14T06:19:39.145459Z"}},"outputs":[{"name":"stdout","text":"Total ROOT files found: 85\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Inspect ROOT Tree Structure\n\nCMS NanoAOD typically stores event-level observables in:\n\n- \"Events\"\n\nHowever, MiniAOD or derived samples may differ.\n\nWe confirm the correct tree name before loading features.\n","metadata":{}},{"cell_type":"code","source":"with uproot.open(root_files[0]) as file:\n    print(file.keys())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:19:39.147022Z","iopub.execute_input":"2026-02-14T06:19:39.147916Z","iopub.status.idle":"2026-02-14T06:19:39.230950Z","shell.execute_reply.started":"2026-02-14T06:19:39.147892Z","shell.execute_reply":"2026-02-14T06:19:39.230267Z"}},"outputs":[{"name":"stdout","text":"['Features;9', 'Features;8']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Define Extended Feature Set (50 Observables)\n\nThese features are:\n\n- Event-level\n- Object-level (leading/subleading)\n- Derived kinematics\n- Angular correlations\n- Ratio features\n\nAll are inference-safe (no generator-level information).\n","metadata":{}},{"cell_type":"code","source":"FEATURES = [\n    'nMuon','nElectron','nJet','MET_pt','MET_phi','MET_sumEt',\n    'Muon_pt_0','Muon_eta_0','Muon_phi_0',\n    'Muon_pt_1','Muon_eta_1','Muon_phi_1',\n    'Electron_pt_0','Electron_eta_0','Electron_phi_0',\n    'Electron_pt_1','Electron_eta_1','Electron_phi_1',\n    'Jet_pt_0','Jet_eta_0','Jet_phi_0',\n    'Jet_pt_1','Jet_eta_1','Jet_phi_1',\n    'Jet_pt_2','Jet_eta_2','Jet_phi_2',\n    'Jet_pt_3','Jet_eta_3','Jet_phi_3',\n    'HT','ST',\n    'M_ll','M_jj_01','M_jj_12',\n    'delta_phi_MET_j0','delta_phi_MET_j1','min_delta_phi_MET_jets',\n    'delta_R_j0_j1','delta_phi_ll','delta_R_ll',\n    'Jet_btagDeepB_0','Jet_btagDeepB_1',\n    'MT_lep_MET','HT_ratio','MET_pt_HT_ratio',\n    'nJet_pt30','Jet_mass_0','LeadLepton_pt','sum_pt_leptons'\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:19:39.231874Z","iopub.execute_input":"2026-02-14T06:19:39.232516Z","iopub.status.idle":"2026-02-14T06:19:39.237091Z","shell.execute_reply.started":"2026-02-14T06:19:39.232484Z","shell.execute_reply":"2026-02-14T06:19:39.236360Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Streaming CMS Data Audit (Memory Safe)\n\nWe avoid concatenating all 85 ROOT files.\n\nInstead:\n- Process one file at a time\n- Iterate in chunks (50k events)\n- Accumulate summary statistics\n- Free memory immediately\n\nThis prevents Kaggle RAM overflow.\n\nThis is the correct large-scale HEP data validation strategy.\n","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nsummary_stats = {\n    \"total_events\": 0,\n    \"missing\": 0,\n    \"infinite\": 0,\n    \"negative_physics\": 0,\n    \"angular_violation\": 0\n}\n\nphysical_positive = [\n    'MET_pt','HT','ST','M_ll','M_jj_01','M_jj_12',\n    'Jet_pt_0','Jet_pt_1','Jet_pt_2','Jet_pt_3',\n    'Muon_pt_0','Muon_pt_1',\n    'Electron_pt_0','Electron_pt_1',\n    'Jet_mass_0'\n]\n\nfor file in tqdm(root_files, desc=\"Processing ROOT files\"):\n    \n    with uproot.open(file)[\"Features\"] as tree:\n        \n        for chunk in tree.iterate(FEATURES, step_size=50000, library=\"pd\"):\n            \n            summary_stats[\"total_events\"] += len(chunk)\n            summary_stats[\"missing\"] += chunk.isnull().sum().sum()\n            summary_stats[\"infinite\"] += np.isinf(chunk).sum().sum()\n            \n            for col in physical_positive:\n                if col in chunk.columns:\n                    summary_stats[\"negative_physics\"] += (chunk[col] < 0).sum()\n            \n            phi_cols = [col for col in chunk.columns if \"phi\" in col]\n            for col in phi_cols:\n                summary_stats[\"angular_violation\"] += (\n                    ((chunk[col] < -math.pi) | (chunk[col] > math.pi)).sum()\n                )\n            \n            del chunk\n\nsummary_stats\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:19:39.239135Z","iopub.execute_input":"2026-02-14T06:19:39.239345Z","iopub.status.idle":"2026-02-14T06:24:27.839103Z","shell.execute_reply.started":"2026-02-14T06:19:39.239325Z","shell.execute_reply":"2026-02-14T06:24:27.838493Z"}},"outputs":[{"name":"stderr","text":"Processing ROOT files: 100%|██████████| 85/85 [04:48<00:00,  3.39s/it]\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'total_events': 120786365,\n 'missing': np.int64(0),\n 'infinite': np.int64(0),\n 'negative_physics': np.int64(3258),\n 'angular_violation': np.int64(55314)}"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Missing Value Audit (Streaming Mode)\n\nSince the dataset is too large to fit in memory,\nwe compute missing values per feature while streaming chunks.\n\nThis ensures:\n- Constant memory usage\n- Accurate global statistics\n- Scalability to >100M events\n","metadata":{}},{"cell_type":"code","source":"missing_per_feature = {f: 0 for f in FEATURES}\n\nfor file in root_files:\n    with uproot.open(file)[\"Features\"] as tree:\n        for chunk in tree.iterate(FEATURES, step_size=50000, library=\"pd\"):\n            \n            null_counts = chunk.isnull().sum()\n            \n            for col in FEATURES:\n                missing_per_feature[col] += int(null_counts[col])\n            \n            del chunk\n\n# Show only features with missing values\nmissing_filtered = {k: v for k, v in missing_per_feature.items() if v > 0}\n\nprint(\"Features with missing values:\")\nmissing_filtered\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:35:57.809340Z","iopub.execute_input":"2026-02-14T06:35:57.809811Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Infinite Value Audit\n\nInfinite values often arise from:\n\n- Division by zero (HT_ratio, MET_pt_HT_ratio)\n- Log transforms\n- Improper normalization\n\nThese break:\n\n- Neural Spline Flow likelihood estimation\n- Standard scaling\n- Model stability\n\nAny presence → data corruption.\n","metadata":{}},{"cell_type":"code","source":"inf_summary = np.isinf(df).sum()\ninf_summary = inf_summary[inf_summary > 0]\nprint(\"Features with infinite values:\")\nprint(inf_summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:24:27.849027Z","iopub.status.idle":"2026-02-14T06:24:27.849296Z","shell.execute_reply.started":"2026-02-14T06:24:27.849181Z","shell.execute_reply":"2026-02-14T06:24:27.849195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Physics Validity Check — Non-Negativity\n\nThe following must never be negative:\n\n- Transverse momenta\n- Invariant masses\n- Scalar sums\n- MET\n\nNegative values imply:\n\n- Reconstruction bug\n- Feature engineering error\n- File corruption\n\nSuch events must be removed.\n","metadata":{}},{"cell_type":"code","source":"physical_positive = [\n    'MET_pt','HT','ST','M_ll','M_jj_01','M_jj_12',\n    'Jet_pt_0','Jet_pt_1','Jet_pt_2','Jet_pt_3',\n    'Muon_pt_0','Muon_pt_1',\n    'Electron_pt_0','Electron_pt_1',\n    'Jet_mass_0'\n]\n\nfor col in physical_positive:\n    if col in df.columns:\n        negatives = (df[col] < 0).sum()\n        if negatives > 0:\n            print(f\"{col} has {negatives} negative values\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:24:27.850103Z","iopub.status.idle":"2026-02-14T06:24:27.850368Z","shell.execute_reply.started":"2026-02-14T06:24:27.850241Z","shell.execute_reply":"2026-02-14T06:24:27.850260Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Angular Boundary Validation\n\nPhysics constraints:\n\n- φ ∈ [-π, π]\n- Δφ ∈ [0, π]\n- ΔR ≥ 0\n\nViolations indicate:\n\n- Improper wrapping\n- Incorrect Δφ calculation\n- Mixing degrees & radians\n\nAngular misdefinition severely harms ML modeling.\n","metadata":{}},{"cell_type":"code","source":"import math\n\nphi_cols = [col for col in df.columns if \"phi\" in col]\n\nfor col in phi_cols:\n    outside = ((df[col] < -math.pi) | (df[col] > math.pi)).sum()\n    if outside > 0:\n        print(f\"{col} has {outside} values outside [-π, π]\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:24:27.851721Z","iopub.status.idle":"2026-02-14T06:24:27.852083Z","shell.execute_reply.started":"2026-02-14T06:24:27.851904Z","shell.execute_reply":"2026-02-14T06:24:27.851939Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Object Multiplicity Consistency\n\nLogical constraints:\n\n- If nMuon = 0 → Muon_pt_0 must be 0\n- If nJet < 4 → Jet_pt_3 must be 0\n\nViolations indicate:\n\n- Improper zero-padding\n- Misaligned indexing\n- Incorrect object extraction\n\nThis introduces fake correlations into anomaly detection.\n","metadata":{}},{"cell_type":"code","source":"inconsistency_muon = df[(df['nMuon'] == 0) & (df['Muon_pt_0'] > 0)]\nprint(\"Muon inconsistency count:\", len(inconsistency_muon))\n\ninconsistency_jet = df[(df['nJet'] < 4) & (df['Jet_pt_3'] > 0)]\nprint(\"Jet inconsistency count:\", len(inconsistency_jet))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:24:27.852889Z","iopub.status.idle":"2026-02-14T06:24:27.853143Z","shell.execute_reply.started":"2026-02-14T06:24:27.853001Z","shell.execute_reply":"2026-02-14T06:24:27.853013Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Derived Feature Recalculation Audit\n\nWe recompute HT_ratio and compare with stored value.\n\nLarge mismatches indicate:\n\n- Inconsistent computation across files\n- Mixing NanoAOD and MiniAOD definitions\n- Precision drift\n\nDerived variables should always be recomputed during DCP.\n","metadata":{}},{"cell_type":"code","source":"recalc_ratio = df['HT'] / (df['HT'] + df['MET_pt'] + 1e-9)\ndiff = np.abs(recalc_ratio - df['HT_ratio'])\nprint(\"HT_ratio mismatch count:\", (diff > 1e-3).sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:24:27.856167Z","iopub.status.idle":"2026-02-14T06:24:27.856567Z","shell.execute_reply.started":"2026-02-14T06:24:27.856407Z","shell.execute_reply":"2026-02-14T06:24:27.856441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport uproot\nimport awkward as ak\nfrom tqdm import tqdm\n\n# ======================================================\n# CONFIGURATION\n# ======================================================\n\ninput_dir = \"/kaggle/input/cms2016g29-5785\"\noutput_dir = \"/kaggle/working/processed_events\"\ntree_name = \"Features\"\nbatch_size = \"100 MB\"\n\nos.makedirs(output_dir, exist_ok=True)\n\n# ======================================================\n# CLEANING CONTROL VARIABLES\n# (Used only to decide which events to remove)\n# ======================================================\n\nprotected_fields = [\n    \"Muon_pt_0\",\n    \"LeadLepton_pt\",\n    \"sum_pt_leptons\",\n    \"ST\",\n    \"HT\",\n    \"MET_pt\",\n    \"MT_lep_MET\",\n    \"M_ll\",\n    \"Jet_pt_0\",\n]\n\n# ======================================================\n# CLEANING FUNCTION\n# ======================================================\n\ndef build_clean_mask(events):\n\n    # Start with all True mask\n    first_field = events.fields[0]\n    mask = ak.ones_like(events[first_field], dtype=bool)\n\n    for field in protected_fields:\n\n        if field not in events.fields:\n            continue\n\n        arr = events[field]\n\n        # Finite check\n        mask = mask & np.isfinite(arr)\n\n        # Upper bound protection\n        upper_limits = {\n            \"Muon_pt_0\": 5000,\n            \"LeadLepton_pt\": 5000,\n            \"sum_pt_leptons\": 10000,\n            \"ST\": 20000,\n            \"HT\": 20000,\n            \"MET_pt\": 5000,\n            \"MT_lep_MET\": 10000,\n            \"M_ll\": 20000,\n            \"Jet_pt_0\": 10000,\n        }\n\n        for field in protected_fields:\n\n            if field not in events.fields:\n                continue\n\n            arr = events[field]\n\n            mask = mask & np.isfinite(arr)\n            mask = mask & (arr < upper_limits[field])\n\n\n    return mask\n\n\n# ======================================================\n# MAIN PROCESSING LOOP\n# ======================================================\n\nroot_files = sorted(glob.glob(os.path.join(input_dir, \"*.root\")))\n\nfor filepath in tqdm(root_files, desc=\"Processing ROOT files\"):\n\n    filename = os.path.basename(filepath)\n    output_path = os.path.join(output_dir, filename)\n\n    total_before = 0\n    total_after = 0\n\n    with uproot.recreate(\n        output_path,\n        compression=uproot.LZ4(4)\n    ) as outfile:\n\n        first_batch = True\n\n        for events in uproot.iterate(\n            f\"{filepath}:{tree_name}\",\n            library=\"ak\",          # ✅ Read ALL branches\n            step_size=batch_size\n        ):\n\n            total_before += len(events)\n\n            # Build mask using only protected physics variables\n            mask = build_clean_mask(events)\n\n            cleaned = events[mask]\n            total_after += len(cleaned)\n\n            if len(cleaned) == 0:\n                continue\n\n            # Write ALL branches (full schema preserved)\n            cleaned_dict = {field: cleaned[field] for field in cleaned.fields}\n\n            if first_batch:\n                outfile[tree_name] = cleaned_dict\n                first_batch = False\n            else:\n                outfile[tree_name].extend(cleaned_dict)\n\n    print(f\"\\nFile: {filename}\")\n    print(f\"Original events : {total_before}\")\n    print(f\"Cleaned events  : {total_after}\")\n    if total_before > 0:\n        removed_pct = 100 * (1 - total_after / total_before)\n        print(f\"Removed         : {removed_pct:.2f}%\")\n\nprint(\"\\nAll ROOT files cleaned successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T06:24:27.857736Z","iopub.status.idle":"2026-02-14T06:24:27.858177Z","shell.execute_reply.started":"2026-02-14T06:24:27.857986Z","shell.execute_reply":"2026-02-14T06:24:27.858007Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Cleaning Summary (DCP Audit)\n\nThis notebook checks:\n\n✔ Missing values  \n✔ Infinite values  \n✔ Physical validity  \n✔ Angular constraints  \n✔ Object multiplicity consistency  \n✔ Derived feature correctness   \n\nA dataset is considered clean only if:\n\n- All checks return zero violations\n- Derived features are consistent\n- No schema drift across files\n\nNext steps:\n- Remove invalid events\n- Recompute derived features\n- Apply scaling only after cleaning\n","metadata":{}}]}